{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from typing import NoReturn\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes: list) -> NoReturn:\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    \n",
    "    def feedforward(self, a: np.ndarray) -> np.ndarray:\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = self.sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    \n",
    "    # Função de Ativação Sigmóide\n",
    "    def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "    \n",
    "\n",
    "    def SGD(self, training_data: list, epochs: int, mini_batch_size: int, eta: float, test_data: list = None) -> NoReturn:\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "\n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(f\"Epoch {j}: {self.evaluate(test_data)} / {n_test}\")\n",
    "            else:\n",
    "                print(f\"Epoch {j} complete\")\n",
    "\n",
    "\n",
    "    def backprop(\n",
    "            self,\n",
    "            x: np.ndarray,\n",
    "            y: np.ndarray\n",
    "    ) -> tuple:\n",
    "        \"\"\"\n",
    "        Retorna uma tupla (nabla_b, nabla_w) representando o\n",
    "        gradiente para a função de custo C_x. nabla_b e nabla_w\n",
    "        são listas de arrays numpy, semelhantes a self.biases\n",
    "        e self.weights.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Vetor de entrada\n",
    "            y (np.ndarray): Vetor de saída\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Tupla contendo os vetores de bias e pesos.\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        # Feedforward\n",
    "        activation = x\n",
    "\n",
    "        # Lista para armazenar todas as ativações, camada por camada\n",
    "        activations = [x]\n",
    "\n",
    "        # Lista para armazenar todos os vetores z, camada por camada\n",
    "        zs = []\n",
    "\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "\n",
    "            activation = self.sigmoid(z)\n",
    "            activations.append(activation)\n",
    "\n",
    "        # Backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * self.sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "\n",
    "        # Aqui, l = 1 significa a última camada de neurônios, l = 2 é a segunda e assim por diante.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = self.sigmoid_prime(z)\n",
    "\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        \n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases] # Inicializa os vetores de bias\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] # Atualiza os vetores de bias\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        \n",
    "        self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
